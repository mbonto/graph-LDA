{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varied-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.covariance import OAS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a415588-bb0e-4590-bc51-be3ae9d93aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE ###\n",
    "dataPath = './wideresnet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-blood",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-array",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boring-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to retrieve the representations of mini-ImageNet images from a wideResNet pretrained with S2M2r.\n",
    "# base_dataset contains the representations of the images used to pretrain the backbone.\n",
    "# val_dataset contains the representations of the images used to select the hyperparameters of the backbone.\n",
    "# novel_dataset contains the representations of the images used to generate few-shot problems. \n",
    "def load_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        labels = [np.full(shape=len(data[key]), fill_value=key) for key in data]\n",
    "        data = [features for key in data for features in data[key]]\n",
    "        dataset = dict()\n",
    "        dataset['data'] = np.stack(data, axis=0)\n",
    "        dataset['labels'] = np.concatenate(labels)\n",
    "        return dataset\n",
    "\n",
    "novel_dataset = load_pickle(dataPath + \"test.pkl\")\n",
    "val_dataset = load_pickle(dataPath + \"val.pkl\")\n",
    "base_dataset = load_pickle(dataPath + \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valid-injury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 600, 640)\n",
      "(16, 600, 640)\n",
      "(64, 600, 640)\n"
     ]
    }
   ],
   "source": [
    "# The data are reshaped in [n_class, elements_per_class, length of the representations].\n",
    "def shape_dataset(dataset):\n",
    "    elements_per_class = 600\n",
    "    data = np.zeros((0, elements_per_class, dataset[\"data\"].shape[1]))\n",
    "    data_labels = np.zeros((0, elements_per_class))\n",
    "    labels = dataset[\"labels\"].copy()\n",
    "    while labels.shape[0] > 0:\n",
    "        indices = np.where(dataset[\"labels\"] == labels[0])[0]\n",
    "        data = np.concatenate([data, np.reshape(dataset[\"data\"][indices,:], (1, elements_per_class, -1))], axis = 0)\n",
    "        data_labels = np.concatenate([data_labels, np.ones((1, elements_per_class)) * labels[0]], axis = 0)\n",
    "        indices = np.where(labels != labels[0])[0]\n",
    "        labels = labels[indices]\n",
    "    return data, data_labels\n",
    "\n",
    "novel_data, novel_labels = shape_dataset(novel_dataset)\n",
    "print(novel_data.shape)\n",
    "val_data, val_labels = shape_dataset(val_dataset)\n",
    "print(val_data.shape)\n",
    "base_data, base_labels = shape_dataset(base_dataset)\n",
    "print(base_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forty-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of a few-shot problem (run).\n",
    "elements_per_class = 600\n",
    "shuffle_indices = np.arange(elements_per_class)\n",
    "\n",
    "def shuffle(data):\n",
    "    global shuffle_indices    \n",
    "    for i in range(data.shape[0]):\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "        data[i,:,:] = data[i,shuffle_indices,:]\n",
    "    return data\n",
    "        \n",
    "# w : number of classes.\n",
    "# k : nombre of training examples per class.\n",
    "# q : nombre of query examples per class (test).\n",
    "def generate_run(w, k, q, data):\n",
    "    data = shuffle(data)\n",
    "    classes = np.random.permutation(np.arange(data.shape[0]))[:w]\n",
    "    dataset = data[classes,:k+q,:]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785ebb2-f9b3-4466-b2a1-31cd6fe37513",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "productive-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(precisions):\n",
    "    return np.mean(precisions), (np.std(precisions) * 1.96 / np.sqrt(len(precisions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy is averaged over several few-shot problems (runs).\n",
    "# w: number of classes.\n",
    "# k: number of training examples per class.\n",
    "# q: number of query examples per class (test).\n",
    "# basis: new basis on which the features of the data samples are projected.\n",
    "# weights:  np.array containing coefficients used to normalize the features of the data samples.\n",
    "def perfs(w, k, q, runs, data, basis=None, weights=None, clf_name=None):\n",
    "    precisions = []\n",
    "    for i in range(runs):\n",
    "        dataset = generate_run(w, k, q, data)\n",
    "        precisions.append(perf_on_one_problem(dataset, k, basis=basis, weights=weights, clf_name=clf_name))\n",
    "    return(stats(precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "changed-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(name):\n",
    "    if name == 'LDA':\n",
    "        oa = OAS(store_precision=False, assume_centered=False)\n",
    "        clf = LinearDiscriminantAnalysis(solver='lsqr', covariance_estimator=oa)\n",
    "    elif name == 'NCM':\n",
    "        clf = NearestCentroid()\n",
    "    elif name == 'LR':\n",
    "        clf = LogisticRegression(random_state=0, max_iter=500)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instrumental-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_on_one_problem(dataset, k, basis=None, weights=None, clf_name=None):\n",
    "    if weights is not None:   \n",
    "        assert basis is not None\n",
    "        dataset = our_preprocess(dataset, basis, weights)\n",
    "\n",
    "    # train the classifier.\n",
    "    clf = classifier(clf_name)\n",
    "    X_train = dataset[:,:k,:]\n",
    "    X_test = dataset[:,k:,:]\n",
    "    y_train = np.zeros((X_train.shape[0], X_train.shape[1]))\n",
    "    y_test = np.zeros((X_test.shape[0], X_test.shape[1]))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        y_train[i] = np.ones(X_train.shape[1]) * i\n",
    "        y_test[i] = np.ones(X_test.shape[1]) * i\n",
    "    X_train = X_train.reshape(-1, X_train.shape[2])\n",
    "    X_test = X_test.reshape(-1, X_test.shape[2]) \n",
    "    y_train = y_train.reshape(-1)\n",
    "    y_test = y_test.reshape(-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-spanish",
   "metadata": {},
   "source": [
    "## Our optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "inclusive-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_preprocess(dataset, basis, weights):\n",
    "    \"\"\"Return data projected into the vectors of basis.\n",
    "    \n",
    "    Parameters:\n",
    "        data  --  matrix of shape [number of samples, length]\n",
    "        basis  --  matrix of shape [length, number of vectors]\n",
    "    \"\"\"\n",
    "    dataset = np.matmul(dataset, basis)\n",
    "    return dataset * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convinced-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Fourier Transform\n",
    "def graph_fourier_transform(GSO):\n",
    "    \"\"\"Return the eigenvectors and eigenvalues of the graph shift operator GSO (e.g. adjacency matrix).\n",
    "    \"\"\"\n",
    "    # Check whether the GSO is symmetric.\n",
    "    assert (GSO == GSO.T).all()\n",
    "    # Compute eigenvalues w and eigenvectors v.\n",
    "    # The eigenvalues in w are sorted in ascending order.\n",
    "    # v[:, i] is the normalized eigenvector corresponding to the eigenvalue w[i].\n",
    "    w, v = np.linalg.eigh(GSO)\n",
    "    return w, v\n",
    "\n",
    "def improved_covariance_matrix(data):\n",
    "    # Remove the mean of the class of each sample.\n",
    "    n_class = data.shape[0]\n",
    "    centered_data = np.zeros_like(data)\n",
    "\n",
    "    for c in range(n_class):\n",
    "        mean = np.mean(data[c], axis=0)\n",
    "        centered_data[c] = data[c] - np.reshape(mean, (1, -1))\n",
    "    \n",
    "    # Compute the covariance matrix using all data samples.\n",
    "    centered_data = centered_data.reshape(-1, centered_data.shape[-1])   \n",
    "    cov = np.matmul(np.transpose(centered_data), centered_data) / (centered_data.shape[0] - 1)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-airport",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39091af6-dd97-40d9-8c3e-491213aed298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "runs = 10000\n",
    "n_shot = 5\n",
    "sigma = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f763d-08e3-47b0-ad03-8b3c83426198",
   "metadata": {},
   "source": [
    "### Without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gothic-jones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.5, 0.15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, clf_name=\"NCM\")\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reliable-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.66, 0.14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR\n",
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, clf_name='LR')\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "controlled-simpson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.81, 0.14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, clf_name='LDA')\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7ed81-2b5d-4f22-9afe-6151e239ba3d",
   "metadata": {},
   "source": [
    "### With preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vietnamese-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph: covariance matrix computed on the base dataset.\n",
    "A = improved_covariance_matrix(base_data)\n",
    "w, v = graph_fourier_transform(A)\n",
    "weights = np.sqrt(np.abs(w) + sigma**2)\n",
    "weights = 1 / weights\n",
    "weights = np.expand_dims(weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "arranged-mustang",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.93, 0.14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ours\n",
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, basis=v, weights=weights, clf_name=\"NCM\")\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "considered-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82.56, 0.13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR + prep\n",
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, basis=v, weights=weights, clf_name='LR')\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "retained-killer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.32, 0.14)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "mean, conf = perfs(5, n_shot, 15, runs, novel_data, basis=v, weights=weights, clf_name='LDA')\n",
    "np.round(mean*100, 2), np.round(conf*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-petroleum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myriam",
   "language": "python",
   "name": "myriam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
