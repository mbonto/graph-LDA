{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "featured-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io as sio\n",
    "from sklearn.covariance import OAS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from matplotlib import pyplot as plt\n",
    "from loader import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incomplete-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE ###\n",
    "brainGraphPath = './neuro/graph.mat'\n",
    "IBCPath = '/bigdisk2/nilearn_data/neurovault/collection_6618/'\n",
    "splitDir = './neuro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-fifth",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-proportion",
   "metadata": {},
   "source": [
    "### Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perfect-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(n_shot, dataset, n_query=15, n_way=2):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        n_shot  --  number of training examples per class.\n",
    "        dataset -- 'train', 'val' or 'test'. Split used to generate the few-shot problems.\n",
    "        n_query  --  number of test examples per class.\n",
    "        n_way  -- number of test examples per class.\n",
    "    \"\"\"\n",
    "    sampler_infos = [1, n_way, n_shot, n_query]\n",
    "    data_loader = get_dataloader(dataset, IBCPath, splitDir, True, sampler_infos=sampler_infos)\n",
    "    return data_loader, sampler_infos\n",
    "\n",
    "\n",
    "def brain_graph():\n",
    "    \"\"\"Return the adjacency matrix of the structural graph of the brain.\n",
    "    \"\"\"\n",
    "    return sio.loadmat(brainGraphPath)['SC_avg56']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-respondent",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "concerned-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(name):\n",
    "    if name == 'LDA':\n",
    "        oa = OAS(store_precision=False, assume_centered=False)\n",
    "        clf = LinearDiscriminantAnalysis(solver='lsqr', covariance_estimator=oa)\n",
    "    elif name == 'NCM':\n",
    "        clf = NearestCentroid()\n",
    "    elif name == 'LR':\n",
    "        clf = LogisticRegression(random_state=0, max_iter=500)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "explicit-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_evaluation(data_loader, sampler_infos, n_task, weights=None, v=None, clf_name=None, show=True):\n",
    "    \"\"\" Return the average accuracy on few-shot tasks.\n",
    "    \n",
    "    Parameters:\n",
    "        data_loader  --  data_loader whose each batch contains a few-shot problem.\n",
    "        sampler_infos  --  list containing [1, n_way, n_shot, n_query], the same as the one used to generate the data_loader.\n",
    "        n_task  --  number of few-shot problems on which the results are averaged.\n",
    "        weights  --  np.array containing coefficients used to normalize the features of the data samples.\n",
    "        v  --  new basis on which the features of the data samples are projected.\n",
    "        clf_name --  'LDA', 'NCM' or 'LR'.\n",
    "    \"\"\"\n",
    "    acc_per_task = []\n",
    "    for i in range(n_task):\n",
    "        if show:\n",
    "            print(i, end='\\r')\n",
    "        acc = task_evaluation(data_loader, sampler_infos, weights, v, clf_name)\n",
    "        acc_per_task.append(acc)\n",
    "    mean, conf = compute_confidence_interval(acc_per_task)\n",
    "    if show:\n",
    "        print('The baseline has an average accuracy of {:.2f}% over {} tasks with 95% confidence interval {:.2f}.'.format(mean*100, n_task, conf*100))\n",
    "    return mean, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "awful-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_evaluation(data_loader, sampler_infos, weights, v, clf_name):\n",
    "    \"\"\"\n",
    "    Return the average accuracy on a few-shot task.\n",
    "    A task contains training samples with known labels and query (test)\n",
    "    samples. The accuracy is the number of times we correctly\n",
    "    predict the labels of the query samples.\n",
    "    \"\"\"\n",
    "    n_way = sampler_infos[1]\n",
    "    n_shot = sampler_infos[2]\n",
    "    n_query = sampler_infos[3]\n",
    "    \n",
    "    # Iterate over the number of episodes. (Here 1.)\n",
    "    for x, y in data_loader: \n",
    "        assert x.shape[0] == n_way * (n_shot + n_query)  ## it is not the case if the data do not contain enough samples per class\n",
    "        # Preprocess the data samples.\n",
    "        x = x.view(x.shape[0], -1).numpy()\n",
    "        if v is not None:\n",
    "            assert weights is not None\n",
    "            # Projection in the spectral space\n",
    "            x = projection(x, v)\n",
    "            # Optimize\n",
    "            x = x * weights\n",
    "\n",
    "        # Split the data into training samples and query samples.\n",
    "        # Be careful: the data have to be sorted by split (train/query) and by classes.\n",
    "        X_train = x[:n_way*n_shot]\n",
    "        X_test = x[n_way*n_shot:]\n",
    "        y_train = y[:n_way*n_shot]\n",
    "        y_test = y[n_way*n_shot:]\n",
    "        del x, y\n",
    "\n",
    "        # Classify\n",
    "        clf = classifier(clf_name)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = clf.score(X_test, y_test)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def projection(data, basis):\n",
    "    \"\"\"Return data projected into the vectors of basis.\n",
    "    \n",
    "    Parameters:\n",
    "        data  --  matrix of shape [number of samples, length]\n",
    "        basis  --  matrix of shape [length, number of vectors]\n",
    "    \"\"\"\n",
    "    return np.matmul(data, basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "significant-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval(data):\n",
    "    \"\"\"\n",
    "    Compute the mean and the 95% confidence interval of the values in data.\n",
    "    \"\"\"\n",
    "    data = 1.0 * np.array(data)\n",
    "    m = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    conf = 1.96 * (std / np.sqrt(len(data)))\n",
    "    return m, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-supply",
   "metadata": {},
   "source": [
    "## Our optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attempted-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_graph(GSO):\n",
    "    degree = np.sum(GSO, axis=1)\n",
    "    degree = degree ** (-1/2)\n",
    "    degree = np.diag(degree)\n",
    "    GSO = np.matmul(np.matmul(degree, GSO), degree)\n",
    "    GSO = (GSO + GSO.T) / 2\n",
    "    return GSO\n",
    "\n",
    "def graph_fourier_transform(GSO):\n",
    "    \"\"\"Return the eigenvectors and eigenvalues of the graph shift operator GSO (e.g. adjacency matrix).\n",
    "    \"\"\"\n",
    "    # Check whether the GSO is symmetric.\n",
    "    assert (GSO == GSO.T).all()\n",
    "    # Compute eigenvalues w and eigenvectors v.\n",
    "    # The eigenvalues in w are sorted in ascending order.\n",
    "    # v[:, i] is the normalized eigenvector corresponding to the eigenvalue w[i].\n",
    "    w, v = np.linalg.eigh(GSO)\n",
    "    return w, v\n",
    "\n",
    "def optimal_weights(w, sigma):\n",
    "    w = np.sqrt(w ** 2 + sigma ** 2)\n",
    "    return 1 / w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-shore",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "analyzed-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "n_task = 10000\n",
    "n_shot = 5\n",
    "n_way = 5\n",
    "n_query = 15\n",
    "data_loader, sampler_infos = generate_data_loader(n_shot, 'test', n_query, n_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7802e39a-fd0c-464a-8f8b-2a9f88ba287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 74.18% over 1000 tasks with 95% confidence interval 0.59.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, clf_name=\"NCM\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be077ad-e9cd-4c31-a2db-918478c3ec61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 78.25% over 1000 tasks with 95% confidence interval 0.62.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, clf_name=\"LR\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dce90d9-82e5-4189-9e36-f5c1c006afeb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 79.89% over 1000 tasks with 95% confidence interval 0.62.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, clf_name=\"LDA\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compatible-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph used for our model\n",
    "GSO = brain_graph()\n",
    "GSO = normalized_graph(GSO)\n",
    "\n",
    "# Our optimization\n",
    "sigma = 0.5\n",
    "w, v = graph_fourier_transform(GSO)\n",
    "coefs = optimal_weights(w, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d16d27d-b4d2-4c3d-9260-95251cf6e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 76.98% over 1000 tasks with 95% confidence interval 0.62.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, weights=coefs, v=v, clf_name=\"NCM\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ade6e54-119d-4dc4-a2d7-9eb7de27784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 79.46% over 1000 tasks with 95% confidence interval 0.64.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, weights=coefs, v=v, clf_name=\"LR\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ff0caf7-6979-41d3-883d-1ed00249abfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline has an average accuracy of 79.46% over 1000 tasks with 95% confidence interval 0.64.\n"
     ]
    }
   ],
   "source": [
    "mean, conf = few_shot_evaluation(data_loader, sampler_infos, n_task, weights=coefs, v=v, clf_name=\"LDA\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51941ffb-f4ed-49e0-a94c-6345954f78a9",
   "metadata": {},
   "source": [
    "# Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f4687-9153-469a-b58f-824384bccee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(11, 10))\n",
    "# ax = plt.axes()\n",
    "# cmap = 'magma_r'\n",
    "# cax = ax.matshow(GSO, cmap=cmap)\n",
    "# ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "# # Add colorbar\n",
    "# cbar = fig.colorbar(cax, ticks=[0.0005, 0.10], extend='max', shrink=.85)\n",
    "# cax.set_clim(0, 0.1)\n",
    "# # Vertically oriented colorbar\n",
    "# cbar.ax.set_yticklabels(['0', '0.10'], fontsize=20)\n",
    "# plt.subplots_adjust(left=.3, top=.99, right=.99, bottom=.275)\n",
    "# plt.savefig('normalized_adjacency.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-mixer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myriam",
   "language": "python",
   "name": "myriam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
